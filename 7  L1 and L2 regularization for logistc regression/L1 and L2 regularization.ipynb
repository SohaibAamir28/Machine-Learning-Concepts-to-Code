{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŸ¢ **WHY DO WE NEED REGULARIZATION?**\n",
        "\n",
        "ğŸ”¹ The Core Problem: Overfitting\n",
        "\n",
        "â€œThe model memorizes the training data instead of learning the pattern.â€\n",
        "\n",
        "Real-Life Analogy:\n",
        "\n",
        "Student memorizes answers â†’ fails new exam\n",
        "\n",
        "ML model memorizes data â†’ fails new data\n",
        "\n",
        "ğŸ”¹ Overfitting Happens When:\n",
        "\n",
        "Model is too complex\n",
        "\n",
        "Too many features\n",
        "\n",
        "Polynomial features\n",
        "\n",
        "Small dataset\n",
        "\n",
        "ğŸ”¹ Symptoms of Overfitting:\n",
        "\n",
        "Training Accuracy\tTest Accuracy\n",
        "\n",
        "Very High\tVery Low âŒ\n",
        "\n",
        "ğŸŸ¢ **WHAT IS REGULARIZATION?**\n",
        "\n",
        "Regularization is a technique that penalizes large weights so the model stays simple.\n",
        "\n",
        "Simple Meaning:\n",
        "\n",
        "Donâ€™t allow the model to become â€œtoo confidentâ€\n",
        "\n",
        "Control how much each feature affects prediction\n",
        "\n",
        "ğŸ”¹ Intuition:\n",
        "\n",
        "Big weights â†’ sharp curves â†’ overfitting\n",
        "\n",
        "Small weights â†’ smoother model â†’ better generalization\n",
        "\n",
        "ğŸŸ¢ **L2 REGULARIZATION**\n",
        "\n",
        "ğŸ”¹ What is L2 Regularization?\n",
        "\n",
        "Adds this penalty to loss:\n",
        "\n",
        "ğœ†\n",
        "âˆ‘\n",
        "ğ‘¤\n",
        "2\n",
        "\n",
        "\n",
        "ğŸ”¹ Intuition\n",
        "\n",
        "â€œL2 gently pulls all weights toward zero, but never fully removes them.â€\n",
        "\n",
        "ğŸ”¹ Effect on Model:\n",
        "\n",
        "âœ” Reduces overfitting\n",
        "\n",
        "âœ” Smooth decision boundary\n",
        "\n",
        "âœ” Keeps all features\n",
        "\n",
        "ğŸ”¹ When to Use L2?\n",
        "\n",
        "When all features are useful\n",
        "\n",
        "When features are correlated"
      ],
      "metadata": {
        "id": "8H7mdNEePoc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0\n",
        ")\n",
        "```\n",
        "ğŸ”¹ What is C?\n",
        "\n",
        "Inverse of regularization strength\n",
        "\n",
        "Smaller C â†’ stronger regularization\n",
        "\n",
        "Larger C â†’ weaker regularization"
      ],
      "metadata": {
        "id": "KYcgr0rSQngL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŸ¢ **L1 REGULARIZATION**\n",
        "\n",
        "ğŸ”¹ What is L1 Regularization?\n",
        "\n",
        "Adds this penalty:\n",
        "\n",
        "ğœ†\n",
        "âˆ‘\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "\n",
        "ğŸ”¹ Intuition (Explain Like This):\n",
        "\n",
        "â€œL1 pushes unnecessary weights to exactly zero.â€\n",
        "\n",
        "ğŸ”¹ Effect on Model:\n",
        "\n",
        "âœ” Feature selection\n",
        "âœ” Sparse model\n",
        "âœ” Removes useless features\n",
        "\n",
        "ğŸ”¹ When to Use L1?\n",
        "\n",
        "When many features are irrelevant\n",
        "\n",
        "When you want feature selection\n",
        "\n",
        "**ğŸ”¹  Basic Code**\n",
        "```\n",
        "model = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=1.0\n",
        ")\n",
        "```\n",
        "âš ï¸ Important Note for Students:\n",
        "\n",
        "L1 needs a special solver (liblinear or saga)"
      ],
      "metadata": {
        "id": "LmGiFyw6Q-Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŸ¢ **PRACTICAL DEMO**\n",
        "\n",
        "ğŸ”¹ Dataset\n",
        "```\n",
        "X = [[20], [30], [40], [50], [60], [70], [80]]\n",
        "y = [0, 0, 0, 1, 1, 1, 1]\n",
        "```\n",
        "ğŸ”¹ Model WITHOUT Regularization (Overfit Risk)\n",
        "```\n",
        "model_no_reg = LogisticRegression(\n",
        "    penalty='none'\n",
        ")\n",
        "```\n",
        "ğŸ”¹ Model WITH L2 Regularization\n",
        "```\n",
        "model_l2 = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=0.5\n",
        ")\n",
        "```\n",
        "ğŸ”¹ Model WITH L1 Regularization\n",
        "```\n",
        "model_l1 = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    C=0.5\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "7yK_4Au-Rj2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Dataset\n",
        "X = [[20], [30], [40], [50], [60], [70], [80]]\n",
        "y = [0, 0, 0, 1, 1, 1, 1]\n",
        "\n",
        "# No Regularization\n",
        "model_none = LogisticRegression(penalty=None, solver='lbfgs')\n",
        "model_none.fit(X, y)\n",
        "\n",
        "# L2 Regularization\n",
        "model_l2 = LogisticRegression(penalty='l2', C=0.5)\n",
        "model_l2.fit(X, y)\n",
        "\n",
        "# L1 Regularization\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.5)\n",
        "model_l1.fit(X, y)\n",
        "\n",
        "print(\"No Regularization Coefficients:\", model_none.coef_)\n",
        "print(\"L2 Coefficients:\", model_l2.coef_)\n",
        "print(\"L1 Coefficients:\", model_l1.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKKHM_tvSbh5",
        "outputId": "d48552f0-08fd-4e67-b895-1122bafe2b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Regularization Coefficients: [[2.27927869]]\n",
            "L2 Coefficients: [[0.46333908]]\n",
            "L1 Coefficients: [[0.01822179]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŸ¢ OUTPUT ANALYSIS & DEEP EXPLANATION\n",
        "\n",
        "ğŸ”¹ OUTPUT\n",
        "\n",
        "No Regularization Coefficients: [[2.27927869]]\n",
        "\n",
        "L2 Coefficients: [[0.46333908]]\n",
        "\n",
        "L1 Coefficients: [[0.01822179]]\n",
        "\n",
        "ğŸ§  FIRST: WHAT IS THIS NUMBER?\n",
        "\n",
        "coef_ means:\n",
        "\n",
        "Weight (w) of the feature in logistic regression\n",
        "\n",
        "Your model equation is:\n",
        "\n",
        "logit(p)=wÃ—X+b\n",
        "\n",
        "Here:\n",
        "\n",
        "X = age (20, 30, 40, â€¦)\n",
        "\n",
        "w = coefficient printed above\n",
        "\n",
        "Bigger w = stronger influence of age\n",
        "\n",
        "ğŸŸ¢ CASE 1: NO REGULARIZATION\n",
        "\n",
        "No Regularization Coefficients: [[2.27927869]]\n",
        "\n",
        "\n",
        "Model is free\n",
        "\n",
        "No penalty\n",
        "\n",
        "It tries to fit data as aggressively as possible\n",
        "\n",
        "Meaning in simple words:\n",
        "\n",
        "â€œAge is VERY important â€” increase age slightly and prediction changes a lot.â€\n",
        "\n",
        "ğŸ¯ Interpretation:\n",
        "\n",
        "Large weight â†’ steep decision boundary\n",
        "\n",
        "High confidence predictions\n",
        "\n",
        "Risk of overfitting if data was noisy\n",
        "\n",
        "ğŸ§’ Student-friendly analogy:\n",
        "\n",
        "Student memorizes answers â†’ confident but fragile\n",
        "\n",
        "ğŸŸ¢ CASE 2: L2 REGULARIZATION\n",
        "\n",
        "L2 Coefficients: [[0.46333908]]\n",
        "\n",
        "ğŸ” What did L2 do?\n",
        "\n",
        "L2 adds this penalty:\n",
        "\n",
        "Î»âˆ‘w\n",
        "2\n",
        "\n",
        "This:\n",
        "\n",
        "Punishes large weights\n",
        "\n",
        "Pulls weights closer to zero\n",
        "\n",
        "BUT never removes them completely\n",
        "\n",
        "ğŸ¯ Interpretation:\n",
        "\n",
        "Age still matters\n",
        "\n",
        "But influence is controlled\n",
        "\n",
        "Model becomes smoother\n",
        "\n",
        "ğŸ§’ Analogy:\n",
        "\n",
        "Teacher says: â€œDonâ€™t overreact â€” stay balancedâ€\n",
        "\n",
        "âœ… Key Teaching Point:\n",
        "\n",
        "L2 does NOT remove features, it shrinks them\n",
        "\n",
        "ğŸŸ¢ CASE 3: L1 REGULARIZATION\n",
        "\n",
        "L1 Coefficients: [[0.01822179]]\n",
        "\n",
        "â€œL1 always makes coefficients exactly zeroâ€\n",
        "\n",
        "âœ… Reality:\n",
        "\n",
        "L1 tries to make them zero\n",
        "If feature is important â†’ it becomes very small, not zero\n",
        "\n",
        "Why is it NOT zero here?\n",
        "\n",
        "Because:\n",
        "\n",
        "Only ONE feature\n",
        "\n",
        "That feature is important\n",
        "\n",
        "Model cannot remove it completely\n",
        "\n",
        "So L1 says:\n",
        "\n",
        "â€œIâ€™ll keep it, but Iâ€™ll reduce its power a LOT.â€\n",
        "\n",
        "ğŸ¯ Interpretation:\n",
        "\n",
        "Feature influence â‰ˆ almost zero\n",
        "\n",
        "Very strong regularization\n",
        "\n",
        "Almost flat decision boundary\n",
        "\n",
        "ğŸ§’ Analogy:\n",
        "\n",
        "â€œIâ€™ll allow you, but barely.â€\n",
        "\n",
        "ğŸŸ¢ COMPARISON TABLE\n",
        "```\n",
        "| Model  | Coefficient | Meaning               |\n",
        "| ------ | ----------- | --------------------- |\n",
        "| No Reg | 2.279       | Very strong influence |\n",
        "| L2     | 0.463       | Controlled influence  |\n",
        "| L1     | 0.018       | Almost ignored        |\n",
        "```"
      ],
      "metadata": {
        "id": "1G87o_t7TOR3"
      }
    }
  ]
}